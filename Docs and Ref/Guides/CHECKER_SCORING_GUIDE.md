# Checker & Scoring æŒ‡å—

æœ¬æ–‡æª”èªªæ˜å¦‚ä½•åœ¨ Normal-OJ ä¸­é–‹ç™¼å’Œä½¿ç”¨è‡ªè¨‚ Checker èˆ‡ Scoring Scriptï¼Œç‚ºé¡Œç›®è¨­è¨ˆéˆæ´»çš„è©•æ¸¬èˆ‡è¨ˆåˆ†æ©Ÿåˆ¶ã€‚

## ğŸ“‹ ç›®éŒ„

- [æ¦‚è¿°](#æ¦‚è¿°)
- [Checker æ©Ÿåˆ¶](#checker-æ©Ÿåˆ¶)
  - [é è¨­ Checker](#é è¨­-checker)
  - [è‡ªè¨‚ Checker](#è‡ªè¨‚-checker)
  - [Interactive Mode Checker](#interactive-mode-checker)
- [Scoring Script](#scoring-script)
  - [é è¨­è¨ˆåˆ†](#é è¨­è¨ˆåˆ†)
  - [è‡ªè¨‚è¨ˆåˆ†è…³æœ¬](#è‡ªè¨‚è¨ˆåˆ†è…³æœ¬)
- [ç¯„ä¾‹](#ç¯„ä¾‹)
- [æœ€ä½³å¯¦è¸](#æœ€ä½³å¯¦è¸)

---

## æ¦‚è¿°

Normal-OJ çš„è©•æ¸¬ç³»çµ±åˆ†ç‚ºå…©å€‹éšæ®µï¼š

1. **Checker éšæ®µ**ï¼šé©—è­‰å­¸ç”Ÿç¨‹å¼çš„è¼¸å‡ºæ˜¯å¦æ­£ç¢º
2. **Scoring éšæ®µ**ï¼šæ ¹æ“šæ¸¬è©¦çµæœè¨ˆç®—æœ€çµ‚åˆ†æ•¸

```mermaid
graph LR
    A[å­¸ç”Ÿç¨‹å¼åŸ·è¡Œ] --> B[ç”¢ç”Ÿè¼¸å‡º]
    B --> C[Checker æª¢æŸ¥]
    C --> D{çµæœ}
    D -->|AC/WA/...| E[Scoring è¨ˆåˆ†]
    E --> F[æœ€çµ‚åˆ†æ•¸]
```

---

## Checker æ©Ÿåˆ¶

### é è¨­ Checker

é è¨­ Checker ä½¿ç”¨**é€è¡Œå­—ä¸²æ¯”å°**ï¼Œå¿½ç•¥è¡Œå°¾ç©ºç™½èˆ‡æª”æ¡ˆå°¾ç©ºè¡Œã€‚

**æ¯”å°è¦å‰‡ï¼š**
1. ç§»é™¤æ¯è¡Œå°¾ç«¯çš„ç©ºç™½å­—å…ƒ
2. ç§»é™¤æª”æ¡ˆçµå°¾çš„ç©ºè¡Œ
3. é€è¡Œæ¯”è¼ƒå­¸ç”Ÿè¼¸å‡ºèˆ‡æ¨™æº–ç­”æ¡ˆ

**é©ç”¨å ´æ™¯ï¼š**
- ç­”æ¡ˆå”¯ä¸€ä¸”æ ¼å¼å›ºå®š
- ä¸éœ€è¦å®¹éŒ¯è™•ç†
- ç°¡å–®çš„è¼¸å…¥è¼¸å‡ºé¡Œç›®

---

### è‡ªè¨‚ Checker

ç•¶é¡Œç›®éœ€è¦æ›´è¤‡é›œçš„ç­”æ¡ˆé©—è­‰é‚è¼¯æ™‚ï¼ˆå¦‚æµ®é»æ•¸èª¤å·®ã€å¤šé‡è§£ç­”ã€ç‰¹æ®Šæ ¼å¼ï¼‰ï¼Œå¯ä½¿ç”¨è‡ªè¨‚ Checkerã€‚

#### Checker è¦ç¯„

**æª”æ¡ˆåç¨±ï¼š** `checker.py`

**åŸ·è¡Œç’°å¢ƒï¼š** Python 3

**è¼¸å…¥åƒæ•¸ï¼š**
Checker æœƒä»¥å‘½ä»¤åˆ—åƒæ•¸æ¥æ”¶ä»¥ä¸‹æª”æ¡ˆè·¯å¾‘ï¼š
```python
import sys

input_file = sys.argv[1]      # æ¸¬è³‡è¼¸å…¥æª” (ssttnn.in)
output_file = sys.argv[2]     # å­¸ç”Ÿè¼¸å‡ºæª” (output.txt)
answer_file = sys.argv[3]     # æ¨™æº–ç­”æ¡ˆæª” (ssttnn.out)
```

**è¼¸å‡ºè¦æ±‚ï¼š**
Checker å¿…é ˆåœ¨åŸ·è¡Œå®Œç•¢å¾Œè¼¸å‡ºåˆ¤å®šçµæœåˆ° stdoutï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```
STATUS: <status>
MESSAGE: <message>
```

**Status å€¼ï¼š**
- `AC` - Acceptedï¼ˆç­”æ¡ˆæ­£ç¢ºï¼‰
- `WA` - Wrong Answerï¼ˆç­”æ¡ˆéŒ¯èª¤ï¼‰

**ç¯„ä¾‹è¼¸å‡ºï¼š**
```
STATUS: AC
MESSAGE: All test cases passed
```
æˆ–
```
STATUS: WA
MESSAGE: Expected 42 but got 43 on line 3
```

#### Checker æ¨¡æ¿

```python
#!/usr/bin/env python3
import sys

def check(input_file, output_file, answer_file):
    """
    è‡ªè¨‚ Checker é‚è¼¯
    
    Args:
        input_file: æ¸¬è³‡è¼¸å…¥æª”è·¯å¾‘
        output_file: å­¸ç”Ÿè¼¸å‡ºæª”è·¯å¾‘
        answer_file: æ¨™æº–ç­”æ¡ˆæª”è·¯å¾‘
    
    Returns:
        tuple: (status, message)
               status: "AC" æˆ– "WA"
               message: è©³ç´°è¨Šæ¯
    """
    try:
        # è®€å–æª”æ¡ˆ
        with open(input_file, 'r') as f:
            input_data = f.read()
        
        with open(output_file, 'r') as f:
            output_data = f.read()
        
        with open(answer_file, 'r') as f:
            answer_data = f.read()
        
        # åœ¨é€™è£¡å¯¦ä½œæ‚¨çš„æª¢æŸ¥é‚è¼¯
        # ç¯„ä¾‹ï¼šç°¡å–®çš„å­—ä¸²æ¯”å°
        if output_data.strip() == answer_data.strip():
            return "AC", "Correct answer"
        else:
            return "WA", "Output does not match expected answer"
    
    except Exception as e:
        return "WA", f"Checker error: {str(e)}"

if __name__ == "__main__":
    if len(sys.argv) != 4:
        print("STATUS: WA")
        print("MESSAGE: Invalid checker arguments")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = sys.argv[2]
    answer_file = sys.argv[3]
    
    status, message = check(input_file, output_file, answer_file)
    
    print(f"STATUS: {status}")
    print(f"MESSAGE: {message}")
```

#### é€²éšç¯„ä¾‹ï¼šæµ®é»æ•¸æ¯”å°

```python
def check_float_array(input_file, output_file, answer_file, epsilon=1e-6):
    """æª¢æŸ¥æµ®é»æ•¸é™£åˆ—ï¼Œå…è¨±èª¤å·®"""
    with open(output_file, 'r') as f:
        output_nums = list(map(float, f.read().split()))
    
    with open(answer_file, 'r') as f:
        answer_nums = list(map(float, f.read().split()))
    
    if len(output_nums) != len(answer_nums):
        return "WA", f"Expected {len(answer_nums)} numbers, got {len(output_nums)}"
    
    for i, (out, ans) in enumerate(zip(output_nums, answer_nums)):
        if abs(out - ans) > epsilon:
            return "WA", f"Number {i+1}: expected {ans}, got {out} (diff={abs(out-ans)})"
    
    return "AC", "All numbers within tolerance"
```

#### ä¸Šå‚³ Checker

åœ¨é¡Œç›®ç·¨è¼¯é é¢ï¼š
1. é–‹å•Ÿã€ŒCustom Checkerã€é¸é …
2. ä¸Šå‚³ `checker.py` æª”æ¡ˆ
3. Sandbox æœƒåœ¨è©•æ¸¬æ™‚è‡ªå‹•ä½¿ç”¨æ‚¨çš„ Checker

---

### Interactive Mode Checker

Interactive æ¨¡å¼ä¸­ï¼ŒChecker ç”±æ•™å¸«ç¨‹å¼è² è²¬ï¼Œé€é `Check_Result` æª”æ¡ˆå›å ±çµæœã€‚

**Check_Result æ ¼å¼ï¼š**
```
STATUS: AC
MESSAGE: Correct solution with optimal steps
```

è©³è¦‹ [INTERACTIVE_MODE_FLOW.md](INTERACTIVE_MODE_FLOW.md)ã€‚

> **æ³¨æ„ï¼š** Interactive æ¨¡å¼ä¸‹ç„¡æ³•ä½¿ç”¨ Custom Checkerï¼ˆ`checker.py`ï¼‰ï¼Œåˆ¤å®šé‚è¼¯å¿…é ˆåœ¨æ•™å¸«ç¨‹å¼ä¸­å¯¦ä½œã€‚

---

## Scoring Script

### é è¨­è¨ˆåˆ†

é è¨­è¨ˆåˆ†æ©Ÿåˆ¶ï¼š
1. **Subtask è¨ˆåˆ†**ï¼šæ¯å€‹ subtask å…§æ‰€æœ‰ case éƒ½ AC æ‰å¾—åˆ†
2. **ç¸½åˆ†è¨ˆç®—**ï¼šæ‰€æœ‰ subtask åˆ†æ•¸ç¸½å’Œ

**ç¯„ä¾‹ï¼š**
```
Subtask 1: 3 cases, 30 åˆ†
Subtask 2: 5 cases, 40 åˆ†
Subtask 3: 2 cases, 30 åˆ†

å­¸ç”Ÿçµæœï¼š
- Subtask 1: å…¨ AC â†’ 30 åˆ†
- Subtask 2: 1 å€‹ WA â†’ 0 åˆ†
- Subtask 3: å…¨ AC â†’ 30 åˆ†

ç¸½åˆ† = 30 + 0 + 30 = 60 åˆ†
```

---

### è‡ªè¨‚è¨ˆåˆ†è…³æœ¬

ç•¶éœ€è¦æ›´è¤‡é›œçš„è¨ˆåˆ†é‚è¼¯æ™‚ï¼ˆå¦‚éƒ¨åˆ†çµ¦åˆ†ã€åŠ æ¬Šè¨ˆåˆ†ã€æ™‚é–“åŠ åˆ†ï¼‰ï¼Œå¯ä½¿ç”¨è‡ªè¨‚ Scoring Scriptã€‚

#### Scoring Script è¦ç¯„

**æª”æ¡ˆåç¨±ï¼š** `score.py`

**åŸ·è¡Œç’°å¢ƒï¼š** Python 3

**è¼¸å…¥ï¼š** JSON æ ¼å¼çš„è©•æ¸¬çµæœï¼ˆé€é stdinï¼‰

**è¼¸å…¥ JSON Schemaï¼š**
```json
{
  "submissionId": "01HQABCDEF123456789",
  "problemId": 123,
  "languageType": 1,
  "tasks": [
    {
      "taskIndex": 0,
      "taskScore": 30,
      "caseCount": 3,
      "results": [
        {
          "caseIndex": 0,
          "status": "AC",
          "runTime": 15,
          "memoryUsage": 2048
        },
        {
          "caseIndex": 1,
          "status": "AC",
          "runTime": 20,
          "memoryUsage": 2560
        },
        {
          "caseIndex": 2,
          "status": "WA",
          "runTime": 18,
          "memoryUsage": 2304
        }
      ],
      "subtaskScore": 0
    }
  ],
  "totalScore": 0,
  "staticAnalysis": {
    "status": "success",
    "violations": []
  },
  "lateSeconds": 0,
  "stats": {
    "maxRunTime": 20,
    "avgRunTime": 17.67,
    "sumRunTime": 53,
    "maxMemory": 2560,
    "avgMemory": 2304,
    "sumMemory": 6912
  },
  "checkerArtifacts": {
    "checkResult": "path/to/Check_Result"
  }
}
```

**è¼¸å‡ºï¼š** JSON æ ¼å¼çš„è¨ˆåˆ†çµæœï¼ˆé€é stdoutï¼‰

**è¼¸å‡º JSON Schemaï¼š**
```json
{
  "score": 85,
  "message": "Good performance! -5 for late submission, +10 bonus for efficiency",
  "breakdown": {
    "subtasks": [30, 40, 30],
    "latePenalty": -5,
    "efficiencyBonus": 10
  }
}
```

#### Scoring Script æ¨¡æ¿

```python
#!/usr/bin/env python3
import sys
import json

def calculate_score(data):
    """
    è‡ªè¨‚è¨ˆåˆ†é‚è¼¯
    
    Args:
        data: dict, åŒ…å«è©•æ¸¬çµæœçš„å®Œæ•´è³‡æ–™
    
    Returns:
        dict: è¨ˆåˆ†çµæœ
              {
                  "score": int,
                  "message": str,
                  "breakdown": dict (optional)
              }
    """
    total_score = data['totalScore']  # é è¨­è¨ˆåˆ†çµæœ
    message = "Default scoring"
    
    # åœ¨é€™è£¡å¯¦ä½œæ‚¨çš„è¨ˆåˆ†é‚è¼¯
    # ç¯„ä¾‹ï¼šé²äº¤æ‰£åˆ†
    late_seconds = data.get('lateSeconds', 0)
    if late_seconds > 0:
        late_days = late_seconds / 86400
        penalty = int(late_days * 10)  # æ¯å¤©æ‰£ 10 åˆ†
        total_score = max(0, total_score - penalty)
        message = f"Late submission: -{penalty} points"
    
    return {
        "score": total_score,
        "message": message
    }

if __name__ == "__main__":
    try:
        # å¾ stdin è®€å– JSON
        data = json.load(sys.stdin)
        
        # è¨ˆç®—åˆ†æ•¸
        result = calculate_score(data)
        
        # è¼¸å‡ºçµæœ
        print(json.dumps(result, ensure_ascii=False))
    
    except Exception as e:
        # éŒ¯èª¤è™•ç†ï¼šå›å‚³é è¨­åˆ†æ•¸
        print(json.dumps({
            "score": 0,
            "message": f"Scoring error: {str(e)}"
        }))
        sys.exit(1)
```

#### é€²éšç¯„ä¾‹ï¼šéƒ¨åˆ†çµ¦åˆ†

```python
def calculate_score(data):
    """éƒ¨åˆ†çµ¦åˆ†ï¼šæ¯å€‹ case éƒ½çµ¦åˆ†"""
    total_score = 0
    breakdown = []
    
    for task in data['tasks']:
        task_score = task['taskScore']
        case_count = task['caseCount']
        case_score = task_score / case_count
        
        # è¨ˆç®—è©² subtask çš„åˆ†æ•¸
        subtask_earned = 0
        for result in task['results']:
            if result['status'] == 'AC':
                subtask_earned += case_score
        
        total_score += subtask_earned
        breakdown.append({
            "taskIndex": task['taskIndex'],
            "earned": subtask_earned,
            "total": task_score
        })
    
    return {
        "score": int(total_score),
        "message": "Partial credit awarded",
        "breakdown": breakdown
    }
```

#### é€²éšç¯„ä¾‹ï¼šæ™‚é–“æ•ˆç‡åŠ åˆ†

```python
def calculate_score(data):
    """æ ¹æ“šåŸ·è¡Œæ•ˆç‡çµ¦äºˆåŠ åˆ†"""
    base_score = data['totalScore']
    
    # å¦‚æœå…¨å°ï¼Œæª¢æŸ¥æ•ˆç‡
    if base_score == 100:
        max_time = data['stats']['maxRunTime']
        
        # æ™‚é–“ä½æ–¼ 100ms çµ¦åŠ åˆ†
        if max_time < 100:
            bonus = min(10, int((100 - max_time) / 10))
            return {
                "score": min(100, base_score + bonus),
                "message": f"Efficiency bonus: +{bonus} points",
                "breakdown": {
                    "base": base_score,
                    "bonus": bonus,
                    "maxTime": max_time
                }
            }
    
    return {
        "score": base_score,
        "message": "Standard scoring"
    }
```

#### ä¸Šå‚³ Scoring Script

åœ¨é¡Œç›®ç·¨è¼¯é é¢ï¼š
1. é–‹å•Ÿã€ŒCustom Scoringã€é¸é …
2. ä¸Šå‚³ `score.py` æª”æ¡ˆ
3. Sandbox æœƒåœ¨è©•æ¸¬å®Œæˆå¾ŒåŸ·è¡Œæ‚¨çš„ Scoring Script

---

## ç¯„ä¾‹

### ç¯„ä¾‹ 1ï¼šåœ–è«–é¡Œç›®ï¼ˆå¤šè§£ï¼‰

æŸäº›åœ–è«–é¡Œç›®å¯èƒ½æœ‰å¤šå€‹æ­£ç¢ºç­”æ¡ˆï¼Œéœ€è¦è‡ªè¨‚ Checker é©—è­‰ç­”æ¡ˆçš„æ­£ç¢ºæ€§è€Œéå®Œå…¨åŒ¹é…ã€‚

**checker.py:**
```python
def check_graph_path(input_file, output_file, answer_file):
    """æª¢æŸ¥è·¯å¾‘æ˜¯å¦æœ‰æ•ˆï¼ˆä¸è¦æ±‚èˆ‡æ¨™æº–ç­”æ¡ˆç›¸åŒï¼‰"""
    # è®€å–åœ–çš„çµæ§‹
    with open(input_file, 'r') as f:
        n, m = map(int, f.readline().split())
        edges = []
        for _ in range(m):
            u, v = map(int, f.readline().split())
            edges.append((u, v))
    
    # è®€å–å­¸ç”Ÿçš„è·¯å¾‘
    with open(output_file, 'r') as f:
        path = list(map(int, f.read().split()))
    
    # é©—è­‰è·¯å¾‘æ˜¯å¦æœ‰æ•ˆ
    if len(path) == 0:
        return "WA", "Empty path"
    
    # å»ºç«‹é„°æ¥è¡¨
    graph = {i: [] for i in range(1, n+1)}
    for u, v in edges:
        graph[u].append(v)
        graph[v].append(u)
    
    # æª¢æŸ¥æ¯ä¸€æ­¥æ˜¯å¦ç›¸é„°
    for i in range(len(path) - 1):
        if path[i+1] not in graph[path[i]]:
            return "WA", f"Invalid edge: {path[i]} -> {path[i+1]}"
    
    # æª¢æŸ¥æ˜¯å¦å¾èµ·é»åˆ°çµ‚é»
    if path[0] != 1 or path[-1] != n:
        return "WA", "Path must start at 1 and end at n"
    
    return "AC", "Valid path found"
```

### ç¯„ä¾‹ 2ï¼šæœ€ä½³åŒ–é¡Œç›®ï¼ˆæ™‚é–“æ•ˆç‡è¨ˆåˆ†ï¼‰

**score.py:**
```python
def calculate_score(data):
    """æ ¹æ“šè§£é¡Œé€Ÿåº¦çµ¦åˆ†"""
    if data['totalScore'] < 100:
        return {
            "score": data['totalScore'],
            "message": "Not all test cases passed"
        }
    
    # å…¨å°çš„æƒ…æ³ä¸‹ï¼Œæ ¹æ“šåŸ·è¡Œæ™‚é–“çµ¦åˆ†
    max_time = data['stats']['maxRunTime']
    
    if max_time <= 100:
        score = 100
        tier = "Excellent"
    elif max_time <= 500:
        score = 90
        tier = "Good"
    elif max_time <= 1000:
        score = 80
        tier = "Acceptable"
    else:
        score = 70
        tier = "Slow"
    
    return {
        "score": score,
        "message": f"{tier} performance (max time: {max_time}ms)",
        "breakdown": {
            "tier": tier,
            "maxTime": max_time
        }
    }
```

### ç¯„ä¾‹ 3ï¼šç«¶è³½æ¨¡å¼ï¼ˆAC è¨ˆæ•¸ï¼‰

**score.py:**
```python
def calculate_score(data):
    """ç«¶è³½æ¨¡å¼ï¼šåªè¨ˆç®— AC çš„ case æ•¸é‡"""
    ac_count = 0
    total_count = 0
    
    for task in data['tasks']:
        for result in task['results']:
            total_count += 1
            if result['status'] == 'AC':
                ac_count += 1
    
    # åˆ†æ•¸ = AC æ•¸é‡
    score = ac_count
    
    return {
        "score": score,
        "message": f"{ac_count}/{total_count} test cases passed",
        "breakdown": {
            "ac": ac_count,
            "total": total_count
        }
    }
```

---

## æœ€ä½³å¯¦è¸

### Checker é–‹ç™¼å»ºè­°

1. **éŒ¯èª¤è™•ç†**
   - å§‹çµ‚ä½¿ç”¨ try-except æ•ç²ç•°å¸¸
   - æª”æ¡ˆä¸å­˜åœ¨æ™‚æ‡‰å›å‚³ WA è€Œéå´©æ½°
   - æä¾›æ¸…æ™°çš„éŒ¯èª¤è¨Šæ¯

2. **æ•ˆç‡è€ƒé‡**
   - Checker æœƒåœ¨æ¯å€‹æ¸¬è©¦æ¡ˆä¾‹åŸ·è¡Œï¼Œæ‡‰ä¿æŒé«˜æ•ˆ
   - é¿å…éæ–¼è¤‡é›œçš„æ¼”ç®—æ³•
   - å¤§æª”æ¡ˆä½¿ç”¨ä¸²æµè®€å–

3. **è¨Šæ¯å“è³ª**
   - WA æ™‚æ‡‰æŒ‡å‡ºéŒ¯èª¤ä½ç½®ï¼ˆè¡Œè™Ÿã€æ•¸å€¼ç­‰ï¼‰
   - è¨Šæ¯ç°¡æ½”æ˜ç­ï¼Œå¹«åŠ©å­¸ç”Ÿ debug
   - é¿å…æ´©æ¼æ¨™æº–ç­”æ¡ˆ

4. **æ¸¬è©¦**
   - åœ¨æœ¬åœ°å……åˆ†æ¸¬è©¦ Checker
   - æ¸¬è©¦é‚Šç•Œæƒ…æ³ï¼ˆç©ºè¼¸å‡ºã€æ ¼å¼éŒ¯èª¤ç­‰ï¼‰
   - ç¢ºä¿æ­£ç¢ºç­”æ¡ˆèƒ½é€šé

### Scoring Script é–‹ç™¼å»ºè­°

1. **ç©©å®šæ€§**
   - å¿…é ˆè™•ç†æ‰€æœ‰å¯èƒ½çš„è¼¸å…¥
   - ç™¼ç”ŸéŒ¯èª¤æ™‚æ‡‰æœ‰åˆç†çš„é™ç´šç­–ç•¥
   - ç¢ºä¿ç¸½æ˜¯è¼¸å‡ºæœ‰æ•ˆçš„ JSON

2. **å…¬å¹³æ€§**
   - è¨ˆåˆ†è¦å‰‡æ‡‰äº‹å‰å‘ŠçŸ¥å­¸ç”Ÿ
   - é¿å…éæ–¼è¤‡é›œæˆ–ä¸é€æ˜çš„è¨ˆåˆ†
   - è€ƒæ…®é‚Šç•Œæƒ…æ³çš„è™•ç†

3. **é™¤éŒ¯**
   - ä½¿ç”¨ breakdown æä¾›è©³ç´°çš„è¨ˆåˆ†ç´°ç¯€
   - message æ‡‰èªªæ˜è¨ˆåˆ†é‚è¼¯
   - æœ¬åœ°æ¸¬è©¦æ™‚å¯ä½¿ç”¨ç¯„ä¾‹ JSON

4. **æ•ˆèƒ½**
   - Scoring åœ¨æ‰€æœ‰æ¸¬è©¦å®Œæˆå¾ŒåŸ·è¡Œä¸€æ¬¡
   - å¯ä»¥é€²è¡Œè¼ƒè¤‡é›œçš„è¨ˆç®—
   - é¿å…ç„¡é™è¿´åœˆæˆ–éé•·åŸ·è¡Œæ™‚é–“

### å®‰å…¨æ€§æ³¨æ„äº‹é …

> **è­¦å‘Šï¼š** Checker å’Œ Scoring Script åœ¨ Sandbox ç’°å¢ƒåŸ·è¡Œï¼Œä½†ä»éœ€æ³¨æ„å®‰å…¨

1. **ä¸è¦åŸ·è¡Œå¤–éƒ¨å‘½ä»¤**
   ```python
   # å±éšªï¼ä¸è¦é€™æ¨£åš
   os.system("rm -rf /")
   subprocess.call(["dangerous_command"])
   ```

2. **é™åˆ¶æª”æ¡ˆå­˜å–**
   - åªè®€å–åƒæ•¸æä¾›çš„æª”æ¡ˆ
   - ä¸è¦å¯«å…¥æˆ–ä¿®æ”¹ç³»çµ±æª”æ¡ˆ

3. **è³‡æºé™åˆ¶**
   - Checker å’Œ Scoring æœ‰æ™‚é–“å’Œè¨˜æ†¶é«”é™åˆ¶
   - é¿å…å»ºç«‹å¤§é‡ç‰©ä»¶æˆ–ç„¡é™è¿´åœˆ

4. **ä¸è¦ä¾è³´å¤–éƒ¨å¥—ä»¶**
   - åªä½¿ç”¨ Python æ¨™æº–å‡½å¼åº«
   - é¿å… `import` éæ¨™æº–æ¨¡çµ„

---

## ç–‘é›£æ’è§£

### Checker å¸¸è¦‹å•é¡Œ

**Q: Checker é¡¯ç¤ºã€ŒInvalid checker argumentsã€**

A: æª¢æŸ¥ `sys.argv` é•·åº¦ï¼Œç¢ºä¿æ­£ç¢ºè®€å–ä¸‰å€‹åƒæ•¸

**Q: æ‰€æœ‰æ¸¬è©¦éƒ½é¡¯ç¤º WAï¼Œä½†æœ¬åœ°æ¸¬è©¦æ­£å¸¸**

A: æª¢æŸ¥æª”æ¡ˆè·¯å¾‘ã€ç·¨ç¢¼ã€æ›è¡Œç¬¦è™Ÿæ˜¯å¦æ­£ç¢º

**Q: Checker è¶…æ™‚**

A: å„ªåŒ–æ¼”ç®—æ³•ï¼Œé¿å… O(nÂ²) ä»¥ä¸Šçš„è¤‡é›œåº¦

### Scoring Script å¸¸è¦‹å•é¡Œ

**Q: Scoring æ²’æœ‰åŸ·è¡Œ**

A: æª¢æŸ¥æ˜¯å¦æ­£ç¢ºä¸Šå‚³ `score.py` ä¸¦é–‹å•Ÿ Custom Scoring

**Q: åˆ†æ•¸é¡¯ç¤ºç‚º 0**

A: æª¢æŸ¥ JSON è¼¸å‡ºæ ¼å¼æ˜¯å¦æ­£ç¢ºï¼Œä½¿ç”¨ `json.dumps` ç¢ºä¿æ ¼å¼

**Q: ç„¡æ³•è®€å– staticAnalysis è³‡æ–™**

A: ä½¿ç”¨ `.get()` æ–¹æ³•è™•ç†å¯èƒ½ä¸å­˜åœ¨çš„æ¬„ä½

---

## ç›¸é—œæ–‡æª”

- [API_REFERENCE.md](API_REFERENCE.md) - Backend API åƒè€ƒ
- [INTERACTIVE_MODE_FLOW.md](INTERACTIVE_MODE_FLOW.md) - Interactive æ¨¡å¼èªªæ˜
- [CONFIG_REFERENCE.md](CONFIG_REFERENCE.md) - é¡Œç›®é…ç½®åƒè€ƒ

---

**æœ€å¾Œæ›´æ–°ï¼š** 2025-11-29  
**ç¶­è­·è€…ï¼š** 2025 NTNU Software Engineering Team 1
